{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam13juna/Documents/virtual_envs/mlo/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Libs\n",
    "# =============================================================================\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from os.path import exists\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Transformer\n",
    "# =============================================================================\n",
    "def attention(q, k, v, mask = None, dropout = None):\n",
    "    scores = q.matmul(k.transpose(-2, -1))\n",
    "    scores /= math.sqrt(q.shape[-1])\n",
    "    \n",
    "    #mask\n",
    "    scores = scores if mask is None else scores.masked_fill(mask == 0, -1e3)\n",
    "    \n",
    "    scores = F.softmax(scores, dim = -1)\n",
    "    scores = dropout(scores) if dropout is not None else scores\n",
    "    output = scores.matmul(v)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, out_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "#        self.q_linear = nn.Linear(out_dim, out_dim)\n",
    "#        self.k_linear = nn.Linear(out_dim, out_dim)\n",
    "#        self.v_linear = nn.Linear(out_dim, out_dim)\n",
    "        self.linear = nn.Linear(out_dim, out_dim*3)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.out_dim = out_dim\n",
    "        self.out_dim_per_head = out_dim // n_heads\n",
    "        self.out = nn.Linear(out_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def split_heads(self, t):\n",
    "        return t.reshape(t.shape[0], -1, self.n_heads, self.out_dim_per_head)\n",
    "    \n",
    "    def forward(self, x, y=None, mask=None):\n",
    "        #in decoder, y comes from encoder. In encoder, y=x\n",
    "        y = x if y is None else y\n",
    "        \n",
    "        qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
    "        q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        \n",
    "        #break into n_heads\n",
    "        q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
    "        q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        \n",
    "        #n_heads => attention => merge the heads => mix information\n",
    "        scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "        scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "        out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
    "        \n",
    "        return out\n",
    "\n",
    "    # def forward(self, x, y=None, mask=None):\n",
    "    #     #in decoder, y comes from encoder. In encoder, y=x\n",
    "\n",
    "    #     y = x if y is None else y\n",
    "    #     print('\\n' * 3)\n",
    "\n",
    "    #     print(\"Shape of input:\", x.shape)\n",
    "        \n",
    "    #     # linear projection\n",
    "    #     qkv = self.linear(x) # BS * SEQ_LEN * (3*EMBED_SIZE_L)\n",
    "    #     print(\"Shape after linear projection:\", qkv.shape)\n",
    "        \n",
    "    #     # split into query, key, and value\n",
    "    #     q = qkv[:, :, :self.out_dim] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "    #     k = qkv[:, :, self.out_dim:self.out_dim*2] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "    #     v = qkv[:, :, self.out_dim*2:] # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "    #     print(\"Shape after splitting into query, key, and value:\", q.shape, k.shape, v.shape)\n",
    "        \n",
    "    #     #break into n_heads\n",
    "    #     q, k, v = [self.split_heads(t) for t in (q,k,v)]  # BS * SEQ_LEN * HEAD * EMBED_SIZE_P_HEAD\n",
    "    #     print(\"Shape after splitting into heads:\", q.shape, k.shape, v.shape)\n",
    "        \n",
    "    #     q, k, v = [t.transpose(1,2) for t in (q,k,v)]  # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "    #     print(\"Shape after transposing:\", q.shape, k.shape, v.shape)\n",
    "        \n",
    "    #     #n_heads => attention => merge the heads => mix information\n",
    "    #     scores = attention(q, k, v, mask, self.dropout) # BS * HEAD * SEQ_LEN * EMBED_SIZE_P_HEAD\n",
    "    #     print(\"Shape after attention:\", scores.shape)\n",
    "        \n",
    "    #     scores = scores.transpose(1,2).contiguous().view(scores.shape[0], -1, self.out_dim) # BS * SEQ_LEN * EMBED_SIZE_L\n",
    "    #     print(\"Shape after merging the heads:\", scores.shape)\n",
    "        \n",
    "    #     out = self.out(scores)  # BS * SEQ_LEN * EMBED_SIZE\n",
    "    #     print(\"Shape after output projection:\", out.shape)\n",
    "\n",
    "    #     print('\\n' * 3)\n",
    "        \n",
    "    #     return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, inp_dim, inner_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(inp_dim, inner_dim)\n",
    "        self.linear2 = nn.Linear(inner_dim, inp_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #inp => inner => relu => dropout => inner => inp\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x)))) \n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, n_heads, inner_transformer_size, inner_ff_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(n_heads, inner_transformer_size, dropout)\n",
    "        self.ff = FeedForward(inner_transformer_size, inner_ff_size, dropout)\n",
    "        self.norm1 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.norm2 = nn.LayerNorm(inner_transformer_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x2 = self.norm1(x)\n",
    "        x = x + self.dropout1(self.mha(x2, mask=mask))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout2(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "    # def forward(self, x, mask=None):\n",
    "    #     # apply layer normalization to the input\n",
    "    #     x2 = self.norm1(x)\n",
    "        \n",
    "    #     # apply multi-head attention to the input with the normalized input as query, key, and value\n",
    "    #     # and the given mask (if provided)\n",
    "    #     attn_output = self.mha(x2, mask=mask)\n",
    "        \n",
    "    #     # apply dropout to the attention output and add it to the input (residual connection)\n",
    "    #     x = x + self.dropout1(attn_output)\n",
    "        \n",
    "    #     # apply layer normalization to the output of the previous step\n",
    "    #     x2 = self.norm2(x)\n",
    "        \n",
    "    #     # apply feedforward network to the output of the previous step\n",
    "    #     ff_output = self.ff(x2)\n",
    "        \n",
    "    #     # apply dropout to the feedforward output and add it to the output of the previous step (residual connection)\n",
    "    #     x = x + self.dropout2(ff_output)\n",
    "        \n",
    "    #     # return the final output\n",
    "    #     return x\n",
    "\n",
    "    # def forward(self, x, mask=None):\n",
    "\n",
    "    #     print('\\n' * 3)\n",
    "\n",
    "    #     print(\"Shape of input to encoder layer:\", x.shape)\n",
    "    #     # apply layer normalization to the input\n",
    "    #     x2 = self.norm1(x)\n",
    "    #     print(\"Shape after layer normalization:\", x2.shape)\n",
    "        \n",
    "    #     # apply multi-head attention to the input with the normalized input as query, key, and value\n",
    "    #     # and the given mask (if provided)\n",
    "    #     attn_output = self.mha(x2, mask=mask)\n",
    "    #     print(\"Shape after multi-head attention:\", attn_output.shape)\n",
    "        \n",
    "    #     # apply dropout to the attention output and add it to the input (residual connection)\n",
    "    #     x = x + self.dropout1(attn_output)\n",
    "    #     print(\"Shape after residual connection with attention output:\", x.shape)\n",
    "        \n",
    "    #     # apply layer normalization to the output of the previous step\n",
    "    #     x2 = self.norm2(x)\n",
    "    #     print(\"Shape after second layer normalization:\", x2.shape)\n",
    "        \n",
    "    #     # apply feedforward network to the output of the previous step\n",
    "    #     ff_output = self.ff(x2)\n",
    "    #     print(\"Shape after feedforward network:\", ff_output.shape)\n",
    "        \n",
    "    #     # apply dropout to the feedforward output and add it to the output of the previous step (residual connection)\n",
    "    #     x = x + self.dropout2(ff_output)\n",
    "    #     print(\"Shape after residual connection with feedforward output:\", x.shape)\n",
    "\n",
    "    #     print('\\n' * 3)\n",
    "        \n",
    "    #     # return the final output\n",
    "    #     return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Positional Embedding\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = 80):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pe.requires_grad = False\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.pe[:,:x.size(1)] #x.size(1) = seq_len\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_code, n_heads, embed_size, inner_ff_size, n_embeddings, seq_len, dropout=.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        #model input\n",
    "        self.embeddings = nn.Embedding(n_embeddings, embed_size)\n",
    "        self.pe = PositionalEmbedding(embed_size, seq_len)\n",
    "        \n",
    "        #backbone\n",
    "        encoders = []\n",
    "        for i in range(n_code):\n",
    "            encoders += [EncoderLayer(n_heads, embed_size, inner_ff_size, dropout)]\n",
    "        self.encoders = nn.ModuleList(encoders)\n",
    "        \n",
    "        #language model\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.linear = nn.Linear(embed_size, n_embeddings, bias=False)\n",
    "                \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x = x + self.pe(x)\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Dataset\n",
    "# =============================================================================\n",
    "class SentencesDataset(Dataset):\n",
    "    #Init dataset\n",
    "    def __init__(self, sentences, vocab, seq_len):\n",
    "        dataset = self\n",
    "        \n",
    "        dataset.sentences = sentences\n",
    "        dataset.vocab = vocab + ['<ignore>', '<oov>', '<mask>']\n",
    "        dataset.vocab = {e:i for i, e in enumerate(dataset.vocab)} \n",
    "        dataset.rvocab = {v:k for k,v in dataset.vocab.items()}\n",
    "        dataset.seq_len = seq_len\n",
    "        \n",
    "        #special tags\n",
    "        dataset.IGNORE_IDX = dataset.vocab['<ignore>'] #replacement tag for tokens to ignore\n",
    "        dataset.OUT_OF_VOCAB_IDX = dataset.vocab['<oov>'] #replacement tag for unknown words\n",
    "        dataset.MASK_IDX = dataset.vocab['<mask>'] #replacement tag for the masked word prediction task\n",
    "\n",
    "        print(\"Ignore index is: \", dataset.IGNORE_IDX)\n",
    "        print(\"MASK index is: \", dataset.MASK_IDX)\n",
    "    \n",
    "    \n",
    "    #fetch data\n",
    "    def __getitem__(self, index, p_random_mask=0.15):\n",
    "        dataset = self\n",
    "        \n",
    "        #while we don't have enough word to fill the sentence for a batch\n",
    "        s = []\n",
    "        while len(s) < dataset.seq_len:\n",
    "            s.extend(dataset.get_sentence_idx(index % len(dataset)))\n",
    "            index += 1\n",
    "\n",
    "        # print(\"Shape of s: \", np.array(s).shape, s)\n",
    "        # return s\n",
    "        \n",
    "        #ensure that the sequence is of length seq_len\n",
    "        s = s[:dataset.seq_len]\n",
    "        [s.append(dataset.IGNORE_IDX) for i in range(dataset.seq_len - len(s))] #PAD ok\n",
    "\n",
    "        #apply random mask\n",
    "        s = [(random.choice(s), w) if random.random() < p_random_mask else (w, dataset.IGNORE_IDX) for w in s]\n",
    "        \n",
    "        return {'input': torch.Tensor([w[0] for w in s]).long(),\n",
    "                'target': torch.Tensor([w[1] for w in s]).long()}\n",
    "\n",
    "        # print(\"Shape of s: \", np.array(s).shape, s)\n",
    "\n",
    "        # return {'input': [w[0] for w in s],\n",
    "        #         'target': [w[1] for w in s]}\n",
    "\n",
    "    #return length\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    #get words id\n",
    "    def get_sentence_idx(self, index):\n",
    "        dataset = self\n",
    "        s = dataset.sentences[index]\n",
    "        s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s] \n",
    "        return s\n",
    "\n",
    "# =============================================================================\n",
    "# Methods / Class\n",
    "# =============================================================================\n",
    "def get_batch(loader, loader_iter):\n",
    "    try:\n",
    "        batch = next(loader_iter)\n",
    "    except StopIteration:\n",
    "        loader_iter = iter(loader)\n",
    "        batch = next(loader_iter)\n",
    "    return batch, loader_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# #Init\n",
    "# =============================================================================\n",
    "print('initializing..')\n",
    "batch_size = 1024\n",
    "seq_len = 20\n",
    "embed_size = 128\n",
    "inner_ff_size = embed_size * 4\n",
    "n_heads = 8\n",
    "n_code = 8\n",
    "n_vocab = 40000\n",
    "dropout = 0.1\n",
    "# n_workers = 12\n",
    "\n",
    "#optimizer\n",
    "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading text...\n",
      "tokenizing sentences...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Input\n",
    "# =============================================================================\n",
    "#1) load text\n",
    "print('loading text...')\n",
    "pth = 'training.txt'\n",
    "sentences = open(pth).read().lower().split('\\n')\n",
    "\n",
    "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
    "print('tokenizing sentences...')\n",
    "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
    "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
    "sentences = [[w for w in s if len(w)] for s in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating/loading vocab...\n",
      "creating dataset...\n",
      "Ignore index is:  23945\n",
      "MASK index is:  23947\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#3) create vocab if not already created\n",
    "print('creating/loading vocab...')\n",
    "pth = 'vocab.txt'\n",
    "if not exists(pth):\n",
    "    words = [w for s in sentences for w in s]\n",
    "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
    "    vocab = [w[0] for w in vocab]\n",
    "    open(pth, 'w+').write('\\n'.join(vocab))\n",
    "else:\n",
    "    vocab = open(pth).read().split('\\n')\n",
    "\n",
    "#4) create dataset\n",
    "print('creating dataset...')\n",
    "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
    "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[0]:  tensor([    4,     0,    56, 23947,    15, 23947,    20,  2778,     2,    16,\n",
      "        23947,   101,     5,   321,   613,    12,   216, 23947,   152,    27])\n",
      "Y[0]:  tensor([23945, 23945, 23945,    23, 23945,   472, 23945, 23945, 23945, 23945,\n",
      "            0, 23945, 23945, 23945, 23945, 23945, 23945,    83, 23945, 23945])\n"
     ]
    }
   ],
   "source": [
    "sth = next(iter(data_loader))\n",
    "\n",
    "x, y = sth['input'], sth['target']\n",
    "\n",
    "print(\"X[0]: \", x[0])\n",
    "print(\"Y[0]: \", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model...\n",
      "initializing optimizer and loss...\n",
      "training...\n",
      "it: 0  | loss 10.34  | Δw: 1.125\n",
      "it: 100  | loss 8.08  | Δw: 0.163\n",
      "it: 200  | loss 6.99  | Δw: 0.127\n",
      "it: 300  | loss 6.46  | Δw: 0.135\n",
      "it: 400  | loss 6.26  | Δw: 0.227\n",
      "it: 500  | loss 6.37  | Δw: 0.448\n",
      "it: 600  | loss 6.23  | Δw: 0.6\n",
      "it: 700  | loss 6.22  | Δw: 0.997\n",
      "it: 800  | loss 6.27  | Δw: 1.091\n",
      "it: 900  | loss 6.22  | Δw: 1.501\n",
      "saving embeddings...\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "#init model\n",
    "print('initializing model...')\n",
    "model = Transformer(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
    "model = model.cuda()\n",
    "\n",
    "# =============================================================================\n",
    "# Optimizer\n",
    "# =============================================================================\n",
    "print('initializing optimizer and loss...')\n",
    "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
    "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n",
    "\n",
    "# =============================================================================\n",
    "# Train\n",
    "# =============================================================================\n",
    "print('training...')\n",
    "print_each = 100\n",
    "model.train()\n",
    "batch_iter = iter(data_loader)\n",
    "n_iteration = 1000\n",
    "for it in range(n_iteration):\n",
    "    \n",
    "    #get batch\n",
    "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "    \n",
    "    #infer\n",
    "    masked_input = batch['input']\n",
    "    masked_target = batch['target']\n",
    "    \n",
    "    masked_input = masked_input.cuda(non_blocking=True)\n",
    "    masked_target = masked_target.cuda(non_blocking=True)\n",
    "    output = model(masked_input)\n",
    "    \n",
    "    #compute the cross entropy loss \n",
    "    output_v = output.view(-1,output.shape[-1])\n",
    "    target_v = masked_target.view(-1,1).squeeze()\n",
    "    loss = loss_model(output_v, target_v)\n",
    "    \n",
    "    #compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #apply gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    #print step\n",
    "    if it % print_each == 0:\n",
    "        print('it:', it, \n",
    "              ' | loss', np.round(loss.item(),2),\n",
    "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
    "    \n",
    "    #reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "# Results analysis\n",
    "# =============================================================================\n",
    "print('saving embeddings...')\n",
    "N = 3000\n",
    "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
    "s = [dataset.rvocab[i] for i in range(N)]\n",
    "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
    "\n",
    "\n",
    "print('end')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of masked_input:  torch.Size([1024, 20])\n",
      "Shape of masked_target:  torch.Size([1024, 20])\n",
      "Shape of output:  torch.Size([1, 20])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of masked_input: \", masked_input.shape)\n",
    "print(\"Shape of masked_target: \", masked_target.shape)\n",
    "print(\"Shape of output: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use dataset class to convert a given sentence to a sequence of word ids\n",
    "def get_sentence_idx(dataset, sentence):\n",
    "    s = sentence.lower().split(' ')\n",
    "    s = [dataset.vocab[w] if w in dataset.vocab else dataset.OUT_OF_VOCAB_IDX for w in s] \n",
    "    # pad sentence to seq_len with IGNORE_IDX\n",
    "\n",
    "    if len(s) < dataset.seq_len:\n",
    "        s = s + [dataset.IGNORE_IDX for _ in range(dataset.seq_len - len(s))]\n",
    "    return [s]\n",
    "\n",
    "value = get_sentence_idx(dataset, 'how are you doing hotty')\n",
    "\n",
    "# Convert the sequence of word ids to a tensor\n",
    "value = torch.tensor(value).cuda()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the tensor to the model\n",
    "output = model(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.8674,  6.0883,  5.9454,  ..., -2.2181, -2.0016, -2.0290],\n",
       "         [ 5.8636,  6.0432,  5.7491,  ..., -2.3596, -1.9230, -2.1196],\n",
       "         [ 5.2904,  6.4870,  5.9396,  ..., -2.3815, -2.0468, -2.3251],\n",
       "         ...,\n",
       "         [ 5.8052,  6.9436,  6.2948,  ..., -2.4140, -2.3179, -2.4267],\n",
       "         [ 5.4528,  6.9711,  5.9411,  ..., -2.5149, -2.2402, -2.3242],\n",
       "         [ 5.3851,  6.9934,  6.1899,  ..., -2.3688, -2.2924, -2.3098]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the word id with the highest probability\n",
    "output = output.argmax(dim=2).cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.',\n",
       " '.',\n",
       " '.',\n",
       " \"'\",\n",
       " ',',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Convert the word id to a word\n",
    "output = [dataset.rvocab[i.item()] for i in output[0]]\n",
    "\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('mlo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "166496c61138a32d9d6f298a779727d389a647554eedcb433b9a55285638df27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
