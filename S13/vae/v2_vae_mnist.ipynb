{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam13juna/Documents/virtual_envs/mlo/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/shivam13juna/Documents/virtual_envs/mlo/lib/python3.10/site-packages/pl_bolts/utils/warnings.py:30: UserWarning: You want to use `wandb` which is not installed yet, install it with `pip install wandb`.\n",
      "  stdout_func(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from abc import abstractmethod\n",
    "import numpy as np\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "# from torch import tensor as Tensor\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "from pl_bolts.callbacks import PrintTableMetricsCallback\n",
    "\n",
    "# import partial\n",
    "from functools import partial\n",
    "import pytorch_lightning as pl\n",
    "from torch.optim import Adam\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import LightningDataModule\n",
    "import torchvision\n",
    "\n",
    "from pl_bolts.models.autoencoders.components import (\n",
    "    resnet18_decoder,\n",
    "    resnet18_encoder,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MNISTWithLabelsDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.num_classes = 10\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist_dataset[idx]\n",
    "        label_tensor = torch.tensor(label, dtype=torch.int64)\n",
    "        # print(label_tensor.shape)\n",
    "        # print(image.shape)\n",
    "        # onehot encoding the label\n",
    "        one_hot_label = F.one_hot(label_tensor, num_classes=self.num_classes).float()\n",
    "        \n",
    "        return (image, one_hot_label), label\n",
    "        # return image, label\n",
    "\n",
    "\n",
    "\n",
    "class MNISTDataModule(LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./data\", batch_size: int = 256, num_workers: int = 4):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "            \n",
    "        ])\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        mnist_train = MNIST(self.data_dir, train=True, download=True, transform=self.transform)\n",
    "        mnist_val = MNIST(self.data_dir, train=False, download=True, transform=self.transform)\n",
    "        self.mnist_train = MNISTWithLabelsDataset(mnist_train)\n",
    "        self.mnist_val = MNISTWithLabelsDataset(mnist_val)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "\n",
    "\n",
    "# mnist_data = MNISTDataModule()\n",
    "\n",
    "# mnist_data.setup()\n",
    "\n",
    "# x, y = mnist_data.train_dataloader().dataset[0]\n",
    "\n",
    "# print(x[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn class for printing input shape \n",
    "class myprint_pre(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myprint_pre, self).__init__()\n",
    "    def forward(self, x, text = ''):\n",
    "        # print(\"\\nShape pre-print is: \", x.shape, end='\\t')\n",
    "        return x\n",
    "\n",
    "class myprint_post(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myprint_post, self).__init__()\n",
    "    def forward(self, x, text = ''):\n",
    "        # print(\"Shape post-print is: \", x.shape, end='')\n",
    "        return x\n",
    "\n",
    "\n",
    "class VanillaVAE(BaseVAE):\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.pre_print = myprint_pre()\n",
    "        self.post_print = myprint_post()\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "        self.hd = hidden_dims.copy()\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            # cprint = partial(self.print, text = h_dim)\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    self.pre_print,\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU(),\n",
    "                    self.post_print\n",
    "                    # cprint\n",
    "                )\n",
    "\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        # self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "        self.one_hot_encoded_dense = nn.Linear(10, self.hd[-1])\n",
    "\n",
    "        # Build Decoder\n",
    "\n",
    "        self.encoder = resnet18_encoder(False, False)\n",
    "        self.decoder = resnet18_decoder(\n",
    "                    latent_dim=latent_dim, \n",
    "                    input_height=32, \n",
    "                    first_conv=False, \n",
    "                    maxpool1=False\n",
    "                )\n",
    "\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"\n",
    "        # print(\"Input to encoder is: \", input[0].shape, input[1].shape)\n",
    "        result = self.encoder(input[0])\n",
    "\n",
    "        encoded_dense = self.one_hot_encoded_dense(input[1])\n",
    "        # print(\"\\nResult of encoder is: \", result.shape)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        # print(\"Result of encoder flattened is: \", result.shape)\n",
    "\n",
    "        #concatenate the one hot encoded dense layer with the flattened result\n",
    "        result = torch.cat((result, encoded_dense), dim=1)\n",
    "\n",
    "        # print(\"Result of encoder concatenated is: \", result.shape)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        # print(\"\\nInput to decoder is: \", z.shape)\n",
    "        result = self.decoder_input(z)\n",
    "        # print(\"Result of decoder input is: \", result.shape)\n",
    "        result = result.view(-1, self.hd[-1], 1, 1)\n",
    "        # print(\"Result of decoder input reshaped is: \", result.shape)\n",
    "        result = self.decoder(result)\n",
    "        # print(\"Result of decoder is: \", result.shape)\n",
    "        result = self.final_layer(result)\n",
    "        # print(\"Result of final layer is: \", result.shape)\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        # print(\"\\nInput shape in forward: \", len(input), input[0].shape, input[1].shape)\n",
    "        mu, log_var = self.encode(input)\n",
    "        # print(\"Shape of mu in forward: \", mu.shape)\n",
    "        # print(\"Shape of log_var in forward: \", log_var.shape)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        # print(\"Shape of z (post reparameterize) in forward: \", z.shape)\n",
    "        return  [self.decode(z), input, mu, log_var]\n",
    "\n",
    "    def loss_function(self,\n",
    "                      *args,\n",
    "                      **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = kwargs['M_N'] # Account for the minibatch samples from the dataset\n",
    "\n",
    "        # print(\"\\nShape of recons: \", recons.shape)\n",
    "        # print(\"Shape of input: \", input.shape)\n",
    "\n",
    "        recons_loss =F.mse_loss(recons, input[0])\n",
    "\n",
    "\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()}\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples,\n",
    "                        self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEModel(pl.LightningModule):\n",
    "    def __init__(self, vae, lr: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vae(x)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.vae.encode(x)\n",
    "    \n",
    "    def decode(self, x):\n",
    "        return self.vae.decode(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        results = self.vae(x)\n",
    "        # print(\"X is: \", len(x), x[0].shape, x[1].shape)\n",
    "        # print(\"Results from vae: \", len(results), results[0].shape, results[1].shape)\n",
    "        loss_dict = self.vae.loss_function(*results, M_N=x[0].shape[0])\n",
    "        loss = loss_dict['loss']\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        results = self.vae(x)\n",
    "        # print(\"X is: \", len(x), x[0].shape, x[1].shape)\n",
    "        loss_dict = self.vae.loss_function(*results, M_N=x[0].shape[0])\n",
    "        loss = loss_dict['loss']\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "    def generate(self, x):\n",
    "        return self.vae.generate(x)\n",
    "    \n",
    "    def sample(self, num_samples, current_device):\n",
    "        return self.vae.sample(num_samples, current_device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = PrintTableMetricsCallback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam13juna/Documents/virtual_envs/mlo/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:445: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | vae  | VanillaVAE | 4.5 M \n",
      "------------------------------------\n",
      "4.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.5 M     Total params\n",
      "17.863    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24:  71%|███████▏  | 196/275 [00:04<00:01, 48.79it/s, loss=0.638, v_num=24, train_loss_step=0.601, val_loss_step=0.682, val_loss_epoch=0.653, train_loss_epoch=0.673]"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# Set up the MNIST DataModule\n",
    "mnist_datamodule = MNISTDataModule()\n",
    "\n",
    "# Initialize the VAE\n",
    "in_channels = 1\n",
    "latent_dim = 512 \n",
    "vae = VanillaVAE(in_channels=in_channels, latent_dim=latent_dim)\n",
    "\n",
    "# Initialize the Lightning Module for the VAE\n",
    "vae_model = VAEModel(vae)\n",
    "trainer = Trainer(max_epochs=30, gpus=torch.cuda.device_count())\n",
    "trainer.fit(vae_model, datamodule=mnist_datamodule)\n",
    "\n",
    "# Save the trained VAE model\n",
    "torch.save(vae_model.state_dict(), \"vae_mnist.pth\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_datamodule.mnist_val[4][0][0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_datamodule.mnist_val[4][0][1].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, title=None):\n",
    "    tensor = tensor.detach().cpu()\n",
    "    img = torchvision.transforms.ToPILImage()(tensor)\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # Pause a bit so that plots are updated\n",
    "    \n",
    "vae_model.eval()\n",
    "\n",
    "# Load a sample from the MNIST dataset\n",
    "id = 4\n",
    "mnist_sample = (mnist_datamodule.mnist_val[4][0][0].unsqueeze(0), mnist_datamodule.mnist_val[4][0][1].unsqueeze(0))\n",
    "\n",
    "# Reconstruct the sample using the VAE model\n",
    "reconstructed_sample = vae_model.generate(mnist_sample)\n",
    "\n",
    "# Display the original and reconstructed samples\n",
    "plt.figure()\n",
    "imshow(mnist_sample[0][0], title=\"Original Sample\")\n",
    "plt.figure()\n",
    "imshow(reconstructed_sample[0], title=\"Reconstructed Sample\")\n",
    "\n",
    "# Generate new images from the VAE model\n",
    "num_samples = 5\n",
    "generated_samples = vae_model.sample(num_samples=num_samples, current_device=vae_model.device)\n",
    "\n",
    "# Display the generated samples\n",
    "for i in range(num_samples):\n",
    "    plt.figure()\n",
    "    imshow(generated_samples[i], title=f\"Generated Sample {i + 1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Assuming the VAE model is already loaded as `vae_model`\n",
    "# and the data module is already created as `mnist_data`\n",
    "\n",
    "# Prepare the validation data loader\n",
    "val_loader = mnist_datamodule.val_dataloader()\n",
    "\n",
    "# Initialize empty lists to store the mu values for each class\n",
    "mu_values = [[] for _ in range(10)]\n",
    "labels_list = []\n",
    "\n",
    "# Iterate through the validation dataset\n",
    "for (images, one_hot_labels), labels in val_loader:\n",
    "    # Encode the images to obtain mu\n",
    "    mu, _ = vae_model.encode((images, one_hot_labels))\n",
    "\n",
    "    # Store the mu values for each class\n",
    "    for i in range(len(labels)):\n",
    "        label = labels[i].item()\n",
    "        mu_values[label].append(mu[i].detach().numpy())\n",
    "        labels_list.append(label)\n",
    "\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "mu_values = np.concatenate(mu_values, axis=0)\n",
    "labels_list = np.array(labels_list)\n",
    "\n",
    "# Reduce the dimensionality of the mu values to 2D\n",
    "tsne = TSNE(n_components=2)\n",
    "mu_values_2d = tsne.fit_transform(mu_values)\n",
    "\n",
    "# Create a scatter plot with the 2D mu values, colored by class labels\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "scatter = ax.scatter(mu_values_2d[:, 0], mu_values_2d[:, 1], c=labels_list, cmap=\"coolwarm\")\n",
    "legend = ax.legend(*scatter.legend_elements(), title=\"Classes\")\n",
    "ax.add_artist(legend)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(tensor, title=None):\n",
    "    tensor = tensor.detach().cpu()\n",
    "    img = torchvision.transforms.ToPILImage()(tensor)\n",
    "    plt.imshow(img)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # Pause a bit so that plots are updated\n",
    "\n",
    "# Load the trained VAE model\n",
    "vae = VanillaVAE(in_channels=in_channels, latent_dim=latent_dim)\n",
    "vae_model = VAEModel(vae)\n",
    "vae_model.load_state_dict(torch.load(\"vae_mnist.pth\"))\n",
    "vae_model.eval()\n",
    "\n",
    "# Load a sample from the MNIST dataset\n",
    "mnist_sample = mnist_datamodule.mnist_val[0][0].unsqueeze(0)\n",
    "\n",
    "# Reconstruct the sample using the VAE model\n",
    "reconstructed_sample = vae_model.generate(mnist_sample)\n",
    "\n",
    "# Display the original and reconstructed samples\n",
    "plt.figure()\n",
    "imshow(mnist_sample[0], title=\"Original Sample\")\n",
    "plt.figure()\n",
    "imshow(reconstructed_sample[0], title=\"Reconstructed Sample\")\n",
    "\n",
    "# Generate new images from the VAE model\n",
    "num_samples = 5\n",
    "generated_samples = vae_model.sample(num_samples=num_samples, current_device=vae_model.device)\n",
    "\n",
    "# Display the generated samples\n",
    "for i in range(num_samples):\n",
    "    plt.figure()\n",
    "    imshow(generated_samples[i], title=f\"Generated Sample {i + 1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
